{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Sanchit\n",
      "[nltk_data]     Kumar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "import json\n",
    "import os\n",
    "findspark.init()\n",
    "\n",
    "#Pre requisite for textblob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Global variable. Inititalising Spark session.\n",
    "\n",
    "\"\"\"\n",
    "spark = SparkSession.builder.master('local').appName(\"Data Science Sandbox\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Class to create analytics Sandbox\n",
    "\"\"\"\n",
    "class Data_Science_Sandbox:\n",
    "    \n",
    "    config = None\n",
    "    analytics_sandbox = None\n",
    "    lake_path = None\n",
    "    dirs = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.config = json.load(open(\"config.json\"))\n",
    "        self.analytics_sandbox = self.config['local_paths']['analytics_sandbox']\n",
    "        self.lake_path = self.config['local_paths']['lake_path']\n",
    "        self.dirs = [val[1] for val in os.walk(self.lake_path) if val[0] == self.lake_path][0]\n",
    "        print(spark)\n",
    "        \n",
    "    def join_all(self):\n",
    "        business_df = self.select_business_df()\n",
    "        review_df = self.select_review_df()\n",
    "        user_df = self.select_user_df()\n",
    "        final_df = business_df.join(review_df , (business_df.business_id == review_df.business_id) , how = 'inner').drop(review_df.business_id)\n",
    "        final_df = final_df.join(user_df , (final_df.user_id == user_df.user_id) , how = 'inner').drop(user_df.user_id)\n",
    "        #final_df.show()\n",
    "        self.write_to_file(final_df)\n",
    "        \n",
    "        \n",
    "    def write_to_file(self, df):\n",
    "        file_path = os.path.join(self.analytics_sandbox , 'Analytics_file')\n",
    "        df.coalesce(1).write.csv(file_path , mode  = 'overwrite' , header = True)\n",
    "        print(\"File saved at {}\".format(self.analytics_sandbox))\n",
    "        \n",
    "    \n",
    "    def select_business_df(self):\n",
    "        df = spark.read.parquet(os.path.join(self.lake_path , self.dirs[0]))\n",
    "        business_df = df.select(['business_id','name','categories','state','city','review_count','stars'])\n",
    "        business_df = self.rename_columns(business_df , ['name','categories','stars'] , ['business_name','business_categories','business_stars'])\n",
    "        return business_df\n",
    "    \n",
    "    def select_review_df(self):\n",
    "        df = spark.read.parquet(os.path.join(self.lake_path , self.dirs[5]))\n",
    "        review_df = df.select(['business_id','review_id','user_id','date','text','stars'])\n",
    "        review_df = self.rename_columns(review_df , ['date','stars'] , ['review_date','review_stars'])\n",
    "        review_df.show()\n",
    "        #return review_df\n",
    "    \n",
    "    def select_user_df(self):\n",
    "        df = spark.read.parquet(os.path.join(self.lake_path , self.dirs[7]))\n",
    "        user_df = df.select(['user_id','name','review_count'])\n",
    "        user_df = self.rename_columns(user_df , ['name','review_count'], ['user_name','user_review_count'])\n",
    "        return user_df\n",
    "     \n",
    "    '''\n",
    "    ###     Utillity to drop columns from dataframe    ###\n",
    "    def drop_columns(self, df , columns_to_drop = []):\n",
    "        if columns_to_drop == [] :\n",
    "            print(\"No column to drop\")\n",
    "            return df\n",
    "        else:\n",
    "            cols = ''.join([\"'\" + str(name)+\"',\" for name in columns_to_drop])[:-1]\n",
    "            df = df.drop(cols)\n",
    "            return df\n",
    "        \n",
    "    '''   \n",
    "    ###     Utility to rename column names    ####\n",
    "        \n",
    "    def rename_columns(self, df , old_col_names, new_col_names ):\n",
    "        if len(old_col_names) != len(new_col_names):\n",
    "            print(\"Old column name list and new column name list must have equal number of elements.\\nProcess failed.\\nNo column renamed.\")\n",
    "            return df\n",
    "        else:\n",
    "            for old_name, new_name in zip(old_col_names , new_col_names):\n",
    "                df = df.withColumnRenamed(old_name , new_name)\n",
    "            return df\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5aaaefd396c6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdw\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mData_Science_Sandbox\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect_review_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;31m#dw.join_all()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#dw.select_business_df()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-cabc62b71d7a>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manalytics_sandbox\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_paths'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'analytics_sandbox'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlake_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_paths'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lake_path'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlake_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlake_path\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspark\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dw = Data_Science_Sandbox()\n",
    "    dw.select_review_df()\n",
    "    #dw.join_all()\n",
    "    #dw.select_business_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File at location C://Intel/hello world5\n"
     ]
    }
   ],
   "source": [
    "abc = \"C://Intel/\"\n",
    "print(\"File at location {}hello world{}\".format(abc , 5))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
